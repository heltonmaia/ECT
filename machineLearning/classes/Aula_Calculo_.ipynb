{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula_Calculo_.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxwI-TApYt78"
      },
      "source": [
        "# Estudaremos nessa aula:\n",
        "- Conceitos básicos de  Cálculo para *Machine Learning*\n",
        "\n",
        "- Material de suporte, baseado em: [Book: Dive into Deep Learning](http://d2l.ai/chapter_preliminaries/ndarray.html)\n",
        "- [Youtube vídeo sobre Cálculo](https://youtu.be/WUvTyaaNkzM)\n",
        "- Conhece a biblioteca [Matplotlib](https://matplotlib.org/)\n",
        "- Regra da cadeia [youtube](https://youtu.be/p9xjPa1EVrw)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPsWeJRTZGzz"
      },
      "source": [
        "Em ***Machine Learning***, treinamos modelos, atualizando-os sucessivamente para que eles fiquem cada vez melhores à medida que são inseridos mais e mais dados.\n",
        "Geralmente, melhorar a acurácia de uma rede neural, significa minimizar uma função de perda (*loss function*), que responde à pergunta \"quão ruim seria este modelo?\".\n",
        "Por fim, o que realmente interessa é produzir um modelo com um bom desempenho para dados que nunca foram apresentados ao modelo durante o treinamento. \n",
        "\n",
        "Assim, a tarefa de ajustar (*fitting*) os modelos possuem duas preocupações principais: i) otimização: o processo de ajustar o modelos aos dados apresentados; ii) generalização: a capacidade de produzir modelos cuja validade se estende além do conjunto de dados (*dataset*) utilizados para treiná-los.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJz1_GrtY5G2"
      },
      "source": [
        "#importanto bibliotecas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vfkr-yefcQ8o"
      },
      "source": [
        "# Derivadas e Diferenciação\n",
        "\n",
        "O cálculo de derivadas é essencial para quase todos os algoritmos de otimização no *machine learning*, incluindo sua utilização em *loss functions*, que são diferenciáveis em relaçao aos parâmetros do nosso modelo.\n",
        "\n",
        "Considerando a seguinte função: $f: \\mathbb{R} \\rightarrow \\mathbb{R}$,\n",
        "na qual a entrada e saída são escalares, a derivada de $f$ é definida como:\n",
        "\n",
        "$$f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x+h) - f(x)}{h},$$\n",
        "\n",
        ":eqlabel:`eq_derivada`\n",
        "\n",
        "Se o limite $f'(x)$ existe, é dito que $f$ é diferenciável em $x$. Podemos interpretar a derivada $f'(x)$ in :eqref:`eq_derivada` como a taxa de variação instantânea (*instantaneous rate*) de $f(x)$ em relação a $x$. Essa taxa de variação é baseada na variação $h$ em $x$, se aproximando de $0$.\n",
        "\n",
        "Por exemplo:\n",
        "$u = f(x) = 3x^2-4x$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcHL4Z5mlTBW",
        "outputId": "3b9894e5-b216-4e0b-be1c-41a0a208fdb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#função para cauclular f(x)\n",
        " \n",
        "def f(x):\n",
        "  return 3*x**2-4*x\n",
        "\n",
        "print(f(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oiYbe1Ooljo"
      },
      "source": [
        "Definindo $x=1$ e permitindo que $h$ se aproxime de $0$, o resultado numérico de $\\frac{f(x+h) - f(x)}{h}$ em :eq:`eq_derivada` se apxoxima de $2$.\n",
        "Da mesma forma, veremos que a derivada $u'$ é $2$ quando $x=1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDb440Msom7W"
      },
      "source": [
        "#função para calcular a derivada\n",
        "\n",
        "def derivada(x,h):\n",
        "  return (f(x+h) - f(x))/h\n",
        "\n",
        "(f(1.1)-f(1))/0.1\n",
        "#estrutura de repetição para calcular a derivada\n",
        "h=0.1\n",
        "x=1\n",
        "\n",
        "for i in range(30):\n",
        "  print('h=%.10f x=%d limit=%.10f' % (h, x, derivada(x,h)))\n",
        "  h*=0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHqhr530U5zG"
      },
      "source": [
        "Segue algumas notações úteis para os estudos sobre diferenciação.\n",
        "Dado $y = f(x)$, temos $x$ and $y$ como variáveis independentes, e dependentes da função $f$, respectivamente. As seguintes expressões são equivalentes:\n",
        "\n",
        "$$f'(x) = y' = \\frac{dy}{dx} = \\frac{df}{dx} = \\frac{d}{dx} f(x) = Df(x) = D_x f(x),$$\n",
        "\n",
        "onde os símbolos $\\frac{d}{dx}$ e $D$ são operadores diferencias utilizados para o cálculo das derivadas. Algumas regras de diferenciação podem ser facilmente aplicadas em funções comuns:\n",
        "\n",
        "* $DC = 0$ ($C$ é uma constante),\n",
        "* $Dx^n = nx^{n-1}$ (regra da potência, $n$ é um número real),\n",
        "* $De^x = e^x$,\n",
        "* $D\\ln(x) = 1/x.$\n",
        "\n",
        "Para o cálculo de derivadas em funções simples, algumas regras poder ser aplicadas.\n",
        "Suponha que a $f$ e $g$ são ambas diferenciáveis e $C$ é uma constante, temos:\n",
        "\n",
        "a regra da *constante*\n",
        "\n",
        "$$\\frac{d}{dx} [Cf(x)] = C \\frac{d}{dx} f(x),$$\n",
        "\n",
        "a regar da *soma*\n",
        "\n",
        "$$\\frac{d}{dx} [f(x) + g(x)] = \\frac{d}{dx} f(x) + \\frac{d}{dx} g(x),$$\n",
        "\n",
        "a regra do *produto*\n",
        "\n",
        "$$\\frac{d}{dx} [f(x)g(x)] = f(x) \\frac{d}{dx} [g(x)] + g(x) \\frac{d}{dx} [f(x)],$$\n",
        "\n",
        "e a regra do *quociente*\n",
        "\n",
        "$$\\frac{d}{dx} \\left[\\frac{f(x)}{g(x)}\\right] = \\frac{g(x) \\frac{d}{dx} [f(x)] - f(x) \\frac{d}{dx} [g(x)]}{[g(x)]^2}.$$\n",
        "\n",
        "\n",
        "Pode-se aplicar algumas dessas regras para se calcular a derivada de uma função, por exemplo, para $u' = f'(x) = 3 \\frac{d}{dx} x^2-4\\frac{d}{dx}x = 6x-4$, considerando $x = 1$, tem-se $u' = 2$. Esta derivada também é chamada de *slope* da reta tangente à curva $u = f(x)$ when $x = 1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ifBxfvi9LqR"
      },
      "source": [
        "(Exercício)\n",
        "\n",
        "Utilizando o *matplotlib*, escreva um programa que permita visualizar(plotar) a função original $f(x)= 3x^2 - 4x$ e sua derivada (reta tangente), em um dado ponto, veja a figura exemplo do professor.\n",
        "\n",
        "Obs: Se possível, realizar update dinânico do gráfico, permitindo a visalização em tempo real. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e96xLe029tsa"
      },
      "source": [
        "# Definindo a função principal quadrática\n",
        "def f(x):\n",
        "  return 3*x**2-4*x\n",
        "\n",
        "# Definindo sua derivada\n",
        "def slope(x): \n",
        "  return 6*x-4\n",
        "\n",
        "def derivada(x,h):\n",
        "  return (f(x+h) - f(x))/h\n",
        "\n",
        "# Definindo o dados para o eixo x\n",
        "x = np.linspace(-10,10,1000)\n",
        "\n",
        "# Escolhendo os pontops para plotar a linha tangente\n",
        "x1 = 1\n",
        "y1 = f(x1)\n",
        "\n",
        "# Calculando a linha\n",
        "# y = m*(x - x1) + y1\n",
        "def line(x, x1, y1):\n",
        "  #return slope(x1)*(x - x1) + y1\n",
        "  return derivada(x1,0.1)*(x - x1) + y1\n",
        "\n",
        "# Formatando a reta tangente para visualização\n",
        "xrange = np.linspace(x1-2, x1+2, 100)\n",
        "\n",
        "# Plotando figura\n",
        "plt.plot(x, f(x)) #função \n",
        "plt.scatter(x1, y1, color='C1', s=50) #ponto para tangente\n",
        "plt.plot(xrange, line(xrange, x1, y1), 'C1--', linewidth = 2) #reta tangente\n",
        "plt.xlabel('x') \n",
        "plt.ylabel('f(x)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qzbTjXjM_5j"
      },
      "source": [
        "## Derivadas Parciais\n",
        "\n",
        "No *machine learning* geralmente é necesário lidar com funções que possuem muitas variáveis, portanto, precisamos estender a ideia de diferenciação para múltiplas variáveis.\n",
        "\n",
        "Considerando $y = f(x_1, x_2, \\ldots, x_n)$ uma função com $n$ variáveis. A derivada parcial de $y$ em relação a $x_i$ em $i^\\mathrm{th}$ seria:\n",
        "\n",
        "$$ \\frac{\\partial y}{\\partial x_i} = \\lim_{h \\rightarrow 0} \\frac{f(x_1, \\ldots, x_{i-1}, x_i+h, x_{i+1}, \\ldots, x_n) - f(x_1, \\ldots, x_i, \\ldots, x_n)}{h}.$$\n",
        "\n",
        "Para calcular $\\frac{\\partial y}{\\partial x_i}$, basta considerar $x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n$ como constantes e calcular a derivada de $y$ com respeito a $x_i$.\n",
        "\n",
        "Sobre notações, as seguintes sãão equivalentes:\n",
        "\n",
        "$$\\frac{\\partial y}{\\partial x_i} = \\frac{\\partial f}{\\partial x_i} = f_{x_i} = f_i = D_i f = D_{x_i} f.$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHAEDx2lroP-"
      },
      "source": [
        "## Gradientes\n",
        "\n",
        "- Assistir o vídeo: Gradient Descent and Neural Network Learn: https://youtu.be/IHZwWFHWa-w\n",
        "\n",
        "Pode-se concatenar derivadas parciais em funções com múltiplas variáveis com respeito a essas variáveis, chamamos isso de Gradiente e é extremamente útil no *machine learning*, principalmente para otimização dos algoritmos de *deep learning*.\n",
        "\n",
        "Suponha que uma função de entrada $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ é um vator $n$-dimensional $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]^\\top$ e sua saída é um escalar. O gradiente desta função $f(\\mathbf{x})$ com respeito a $\\mathbf{x}$ é um vetor de $n$ derivadas parciais:\n",
        "\n",
        "$$\\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\bigg[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1}, \\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\ldots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\bigg]^\\top,$$\n",
        "\n",
        "onde o operador $\\nabla_{\\mathbf{x}} f(\\mathbf{x})$ é normalmente substituído por $\\nabla f(\\mathbf{x})$.\n",
        "\n",
        "Dado $\\mathbf{x}$ um vetor $n$-dimensional, as regras a seguir são comumente utilizadas para cálculo de derivadas em funções com múltiplas variáveis:\n",
        "\n",
        "* Para todo $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, $\\nabla_{\\mathbf{x}} \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top$,\n",
        "* Para todo  $\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$, $\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A}  = \\mathbf{A}$,\n",
        "* para todo  $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, $\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x}  = (\\mathbf{A} + \\mathbf{A}^\\top)\\mathbf{x}$,\n",
        "* $\\nabla_{\\mathbf{x}} \\|\\mathbf{x} \\|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2\\mathbf{x}$.\n",
        "\n",
        "De forma similar, para qualquer matriz $\\mathbf{X}$, temm-se $\\nabla_{\\mathbf{X}} \\|\\mathbf{X} \\|_F^2 = 2\\mathbf{X}$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0LIVukRvIhL"
      },
      "source": [
        "## Regra da Cadeis (*Chain Rule*)\n",
        "\n",
        "- Vídeo:\n",
        "\n",
        "Entretando, achar esses gradients pode não ser uma tarefa fácil. Principalmente pelo fato de que em *deep learning* as funções multivariáveis serem funções compostas, então as regras acima não podem ser aplicadas diretamente. Assim, para diferenciação de funções compostas, utilzamos a regra da cadeia (*chain rule*).\n",
        "\n",
        "Considerando inicialmente as funções de uma única variável. Suponha que $y=f(u)$ e $u=g(x)$ sejam diferenciáveis, aplicando a regra da cadeia teríamos: \n",
        "\n",
        "\n",
        "$$\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dx}.$$\n",
        "\n",
        "Em um cenário mais geral onde as funções possuem um número arbitrário de variáveis, suponha que a função diferenciável $y$ tenha as variáveis\n",
        "$u_1, u_2, \\ldots, u_m$, onde cada função diferenciável $u_i$ tenha variáveis $x_1, x_2, \\ldots, x_n$.\n",
        "Note que $y$ é uma função de $x_1, x_2, \\ldots, x_n$.\n",
        "A regra da cadeia fornece:\n",
        "\n",
        "$$\\frac{dy}{dx_i} = \\frac{dy}{du_1} \\frac{du_1}{dx_i} + \\frac{dy}{du_2} \\frac{du_2}{dx_i} + \\cdots + \\frac{dy}{du_m} \\frac{du_m}{dx_i}$$\n",
        "\n",
        "para todo $i = 1, 2, \\ldots, n$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvUjbSoscvY0"
      },
      "source": [
        "## Python: Utilzando variáveis simbólicas com o *sympy*\n",
        "- Vejam a documentação: https://docs.sympy.org/latest/modules/vector/api/vectorfunctions.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ9ICx1_c6Wm",
        "outputId": "bfad9415-92bd-4d1e-851f-754b4ea85f78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# import sympy \n",
        "from sympy import *\n",
        "\n",
        "x, y = symbols('x y') \n",
        "res = x**2 + 2 * y + y**3\n",
        "print(\"Expressão : {} \".format(res)) \n",
        "\n",
        "# Usando o sympy.Derivative() method \n",
        "res_diff_x = diff(res, x) \n",
        "res_diff_y = diff(res, y) \n",
        "\t\n",
        "print(\"Derivada da expressao em x : {}\".format(res_diff_x)) \n",
        "print(\"Derivada da expressao em y : {}\".format(res_diff_y)) "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expressão : x**2 + y**3 + 2*y \n",
            "Derivada da expressao em x : 2*x\n",
            "Derivada da expressao em y : 3*y**2 + 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K0clAjzj-dO"
      },
      "source": [
        "(Exercício) Utilizando a regra da cadeia, calcule a derivada $dy/dx$ da função y = ($3x^2 - 5x + 2)^6$ no papel, depois resolva utilizando variáveis simbólicas para comparar os resultados. Por fim, plote o gráfico da função e de sua reta tangente em um dado ponto.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AfXE-EPVTSx"
      },
      "source": [
        "# Exercicio com sympy\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}